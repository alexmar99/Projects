{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fae02ad",
   "metadata": {},
   "source": [
    "# Web Scraper LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d9a10",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import all the relevant libraries for the project\n",
    "from selenium import webdriver #allow us to initialize and run our selenium driver\n",
    "from selenium.webdriver.common.by import By #gives us functions to find within the scraped elements\n",
    "from bs4 import BeautifulSoup #helps us create a searchable element from the HTML\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608bd3a",
   "metadata": {},
   "source": [
    "# Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We give selenium our user data and make it access LinkedIn\n",
    "driver = webdriver.Chrome()\n",
    "time.sleep(2.5)\n",
    "\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "username = driver.find_element(By.ID,\"username\")\n",
    "\n",
    "username.send_keys(\"\") \n",
    "\n",
    "password = driver.find_element(By.ID,'password')\n",
    "\n",
    "password.send_keys('')\n",
    "\n",
    "driver.find_element(By.XPATH,\"//button[@type='submit']\").click()\n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "#we give the basic link to start the iteration through pages\n",
    "base = 'https://www.linkedin.com/jobs/search/?currentJobId=3361173187&geoId=107025191&keywords=data%20scientist%2C%20barcelona&location=Barcelona%2C%20Catalonia%2C%20Spain&start='\n",
    "\n",
    "action = webdriver.ActionChains(driver)\n",
    "\n",
    "#initialize vectors to append values from the code\n",
    "all_promoted = []\n",
    "easy_applies = []\n",
    "job_titles = []\n",
    "company_titles = []\n",
    "urls = []\n",
    "locations = []\n",
    "states = []\n",
    "posting_dates = []\n",
    "appl_counts = []\n",
    "work_type  = []\n",
    "seniorities  = []\n",
    "emp_type = []\n",
    "industries = []\n",
    "num_employees = []\n",
    "all_followers = []\n",
    "python_required = []\n",
    "i=0 #start at job 0\n",
    "\n",
    "#We create our main loop to iterate through pages\n",
    "while i <= 1000:\n",
    "    \n",
    "#we go to the first page of the job search\n",
    "    driver.get(base+str(i))\n",
    "    time.sleep(2)\n",
    "\n",
    "#we scroll through the job list element\n",
    "    jobs_div = driver.find_element(By.XPATH,'//*[@id=\"main\"]/div/section[1]/div')\n",
    "    scroll_position = 0\n",
    "    height = driver.execute_script(\"return arguments[0].scrollHeight\", jobs_div)\n",
    "\n",
    "    while scroll_position <= height:\n",
    "        scroll_position += 2\n",
    "        driver.execute_script(f\"arguments[0].scrollTo(0, {scroll_position});\", jobs_div)\n",
    "        \n",
    "#we create the item through which we'll iterate for each page\n",
    "    jobs_list = driver.find_elements(By.CSS_SELECTOR, '.jobs-search-results-list .job-card-container')\n",
    "    \n",
    "#we create the way for the code to end when we get to the last page\n",
    "    if len(jobs_list) < 5:\n",
    "        break\n",
    "\n",
    "#we create a loop to go through each posting on the page and extract the data        \n",
    "    for items in jobs_list:\n",
    "#we click on the job\n",
    "        clicker = items.find_element(By.CSS_SELECTOR, '.job-card-container__link')\n",
    "        time.sleep(0.1)\n",
    "        clicker.click()\n",
    "#we create a soup from the small card to get some key data\n",
    "        card_soup = BeautifulSoup(items.get_attribute('innerHTML'),'html.parser')\n",
    "\n",
    "#we scroll down through each page so it loads and we get the entire needed data    \n",
    "        job_div = driver.find_element(By.XPATH,'//*[@id=\"main\"]/div/section[2]/div')\n",
    "        scroll_position = 0\n",
    "        height = driver.execute_script(\"return arguments[0].scrollHeight\", job_div)\n",
    "\n",
    "        while scroll_position <= height:\n",
    "            scroll_position += 2\n",
    "            driver.execute_script(f\"arguments[0].scrollTo(0, {scroll_position});\", job_div)\n",
    "\n",
    "#we create a soup element for all the info in each job            \n",
    "        main_page_soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "#we get the promoted field from the card soup\n",
    "        try:\n",
    "            if card_soup.select_one('.job-card-container__footer-item').text:\n",
    "                promoted_result = card_soup.select_one('.job-card-container__footer-item').text\n",
    "            else:\n",
    "                promoted_result = np.NaN\n",
    "        except:\n",
    "            promoted_result = np.NaN\n",
    "\n",
    "# we get the job title            \n",
    "        try:\n",
    "            if main_page_soup.select_one(\".jobs-unified-top-card__job-title\"):\n",
    "                job_titles.append(main_page_soup.select_one(\".jobs-unified-top-card__job-title\").text)\n",
    "            else:\n",
    "                job_titles.append(np.NaN)\n",
    "        except:\n",
    "            job_titles.append(np.NaN)\n",
    "\n",
    "#we get the url for the job\n",
    "        try:\n",
    "            if card_soup.select_one('.job-card-container__link')['href']:\n",
    "                urls.append('linkedin.com'+card_soup.select_one('.job-card-container__link')['href'])\n",
    "            else:\n",
    "                urls.append(np.NaN)\n",
    "        except:\n",
    "            urls.append(np.NaN)\n",
    "        \n",
    "# we get the company name\n",
    "        try:\n",
    "            if main_page_soup.select_one(\".ember-view.t-black.t-normal\"):\n",
    "                company_titles.append(main_page_soup.select_one(\".ember-view.t-black.t-normal\").text.strip())\n",
    "            else:\n",
    "                company_titles.append(np.NaN)\n",
    "        except:\n",
    "            company_titles.append(np.NaN)\n",
    "            \n",
    "#we review if python is in the requirements            \n",
    "        try:\n",
    "            if main_page_soup.find('div',{'class':'jobs-box--with-cta-large'}):\n",
    "                if 'python' in main_page_soup.find('div',{'class':'jobs-box--with-cta-large'}).text.lower():\n",
    "                    python_required.append(True)\n",
    "                else: \n",
    "                    python_required.append(False)\n",
    "                    \n",
    "            else:\n",
    "                python_required.append(False)\n",
    "        except:\n",
    "            python_required.append(np.NaN)\n",
    "            \n",
    "# we get the location\n",
    "        try:\n",
    "            if main_page_soup.select_one('.jobs-unified-top-card__bullet'):\n",
    "                locations.append(main_page_soup.select_one('.jobs-unified-top-card__bullet').text.strip())\n",
    "            else:\n",
    "                locations.append(np.NaN)\n",
    "        except:\n",
    "            locations.append(np.NaN)\n",
    "\n",
    "#we get the list data that contains seniority, employment type. etc            \n",
    "        try:\n",
    "            if main_page_soup.select('.jobs-unified-top-card__job-insight'):\n",
    "                list_data = main_page_soup.select('.jobs-unified-top-card__job-insight')\n",
    "            else:\n",
    "                list_data = np.NaN\n",
    "        except:\n",
    "            list_data= np.NaN\n",
    "\n",
    "#we get the posting data            \n",
    "        try:    \n",
    "            if main_page_soup.select_one('.jobs-unified-top-card__posted-date'):\n",
    "                posting_dates.append(main_page_soup.select_one('.jobs-unified-top-card__posted-date').text.strip())\n",
    "            else:\n",
    "                posting_dates.append(np.NaN)\n",
    "        except:\n",
    "            posting_dates.append(np.NaN)\n",
    "\n",
    "#we get the companies followers            \n",
    "        try:\n",
    "            if main_page_soup.find('div',{'class':'artdeco-entity-lockup__subtitle ember-view t-16'}):\n",
    "                all_followers.append(main_page_soup.find('div',{'class':'artdeco-entity-lockup__subtitle ember-view t-16'}).text.strip().split()[0])\n",
    "            else:\n",
    "                all_followers.append(np.NaN)\n",
    "        except :\n",
    "            all_followers.append(np.NaN)\n",
    "            \n",
    "#we get the company industry\n",
    "\n",
    "        try:\n",
    "            if main_page_soup.find('div',{'class':'t-14 mt5'}):\n",
    "                industries.append(main_page_soup.find('div',{'class':'t-14 mt5'}).text.strip().split('\\n')[0])\n",
    "            else:\n",
    "                industries.append(np.NaN)\n",
    "        except: \n",
    "            industries.append(np.NaN)\n",
    "\n",
    "#we get the number of employees\n",
    "        try:    \n",
    "            if main_page_soup.find_all('span',{'class':'jobs-company__inline-information'}):\n",
    "                num_employees.append(main_page_soup.find_all('span',{'class':'jobs-company__inline-information'})[1].text.strip().split(' ')[0])\n",
    "            else:\n",
    "                num_employees.append(np.NaN)\n",
    "        except:\n",
    "            num_employees.append(np.NaN)\n",
    "            \n",
    "# we get the workplace type\n",
    "        try:\n",
    "            if main_page_soup.select_one('.jobs-unified-top-card__workplace-type'):\n",
    "                workplace_type = main_page_soup.select_one('.jobs-unified-top-card__workplace-type').text.strip()\n",
    "            else:\n",
    "                workplace_type = np.NaN\n",
    "        except:\n",
    "            workplace_type = np.NaN\n",
    "\n",
    "        work_type.append(workplace_type)\n",
    "\n",
    "#we get the applicant count\n",
    "        try:\n",
    "            if main_page_soup.select_one(\".jobs-unified-top-card__applicant-count\"):\n",
    "                applicant_count = main_page_soup.select_one(\".jobs-unified-top-card__applicant-count\").text.strip()[0]\n",
    "            else:\n",
    "                applicant_count = np.NaN\n",
    "\n",
    "        except:\n",
    "            applicant_count = np.NaN\n",
    "\n",
    "        appl_counts.append(applicant_count)\n",
    "\n",
    "#we go through the list to get employment and seniority\n",
    "        try: \n",
    "            if list_data is not np.NaN:\n",
    "                aux = list_data[0].text.strip().split('Â·')\n",
    "            else:\n",
    "                aux = np.NaN\n",
    "        except:\n",
    "            try:\n",
    "                aux = list_data[0].text.strip()\n",
    "            except:\n",
    "                aux = np.NaN\n",
    "\n",
    "        if type(aux)== list:\n",
    "            if aux == np.NaN:\n",
    "                employment_type = np.NaN\n",
    "                seniority = np.NaN\n",
    "            elif len(aux) ==2:\n",
    "                employment_type = aux[0]\n",
    "                seniority = aux[1]\n",
    "            else:\n",
    "                employment_type=aux[0]\n",
    "                seniority = np.NaN\n",
    "        else: \n",
    "            employment_type=np.NaN\n",
    "            seniority = np.NaN\n",
    "\n",
    "        seniorities.append(seniority)\n",
    "        emp_type.append(employment_type)\n",
    "\n",
    "# we get the state\n",
    "        try:\n",
    "            if driver.find_elements(By.CSS_SELECTOR, '.mt5.mb2 ul li')[-1]:\n",
    "                aux = driver.find_elements(By.CSS_SELECTOR, '.mt5.mb2 ul li')[-1].text\n",
    "                if aux == 'Actively recruiting':\n",
    "                    state = 'On-going'\n",
    "                elif aux == 'Be an early applicant':\n",
    "                    state = 'Early Applications'\n",
    "                else:\n",
    "                    state = 'Other'\n",
    "            else:\n",
    "                state='Other'\n",
    "        except:\n",
    "            state = 'Other'\n",
    "\n",
    "        states.append(state)\n",
    "\n",
    "# we get the promoted label for each job        \n",
    "        try:\n",
    "            if promoted_result == 'Promoted':\n",
    "                promoted = True\n",
    "            else:\n",
    "                promoted = False\n",
    "        except:\n",
    "            promoted = np.NaN\n",
    "\n",
    "        all_promoted.append(promoted)\n",
    "\n",
    "#we get the easy apply status through the buttons text\n",
    "        try:\n",
    "            if card_soup.find('li',{'class':\"job-card-container__apply-method job-card-container__footer-item inline-flex align-items-center\"}):\n",
    "                apply = card_soup.find('li',{'class':\"job-card-container__apply-method job-card-container__footer-item inline-flex align-items-center\"}).text.strip()\n",
    "\n",
    "                if apply == 'Easy Apply':\n",
    "                    easy_apply = True\n",
    "                else:\n",
    "                    easy_apply = False\n",
    "            else:\n",
    "                easy_apply = False\n",
    "\n",
    "        except:\n",
    "            easy_apply = False\n",
    "\n",
    "        easy_applies.append(easy_apply)\n",
    "        \n",
    "    i+=len(jobs_list)\n",
    "\n",
    "result = pd.DataFrame(data={'Job Title':job_titles,\n",
    "                            'Company Name':company_titles,\n",
    "                            'State':states,\n",
    "                            'Location':locations,\n",
    "                            'Workspace':work_type,\n",
    "                            'Posting Date':posting_dates,\n",
    "                            'Seniority Level':seniorities,\n",
    "                            'Industry':industries,\n",
    "                            'Applicants':appl_counts,\n",
    "                            'Employment Type':emp_type,\n",
    "                            'Apply Through LinkedIn':easy_applies,\n",
    "                            'Offer URL': urls,\n",
    "                            'Promoted':all_promoted,\n",
    "                            'Python Required':python_required,\n",
    "                            'Employees':num_employees,\n",
    "                            'Followers':all_followers})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d35686",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
