{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf86a14",
   "metadata": {},
   "source": [
    "# Code for Retrieving All Posts\n",
    "\n",
    "In this section, we will retrieve all posts, including shared posts. The response will be in JSON format, and we will extract the important elements from the response and create a DataFrame with them.\n",
    "\n",
    "## Importing Required Libraries\n",
    "\n",
    "Before we begin, let's import the necessary libraries that we'll be using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fde720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:50.266205Z",
     "start_time": "2023-06-30T11:19:49.954957Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154033a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Loading the Access Token (will be used for all requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397765bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:50.269867Z",
     "start_time": "2023-06-30T11:19:50.267550Z"
    }
   },
   "outputs": [],
   "source": [
    "access_token = \"X\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f0202",
   "metadata": {},
   "source": [
    "## Setting up the headers/parameters/api url to get all the posts\n",
    "\n",
    "Note several things:\n",
    "- `headers_posts`:\n",
    "    - Don't change\n",
    "\n",
    "- `posts_params`:\n",
    "    - `author`: \n",
    "    - `count`: how many results (posts) do we want returned\n",
    "    - `q`: don't change\n",
    "- `posts_url`:\n",
    "    - This is the endpoint to retrieve data on posts (not reactions just posts themselves)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddda68d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:50.273430Z",
     "start_time": "2023-06-30T11:19:50.270872Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "headers_posts = {\n",
    "    'Authorization': f'Bearer {access_token}',\n",
    "    'X-Restli-Protocol-Version': '2.0.0',\n",
    "    'LinkedIn-Version': '202301',\n",
    "    'X-RestLi-Method': 'FINDER',\n",
    "}\n",
    "\n",
    "posts_params = {\n",
    "    'q': 'author',\n",
    "    'author': 'urn:li:organization:X',\n",
    "    'count': '100',\n",
    "}\n",
    "\n",
    "posts_url = f'https://api.linkedin.com/rest/posts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09932651",
   "metadata": {},
   "source": [
    "## Lets define the post data request function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb24a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.688472Z",
     "start_time": "2023-06-30T11:19:50.274938Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'list_posts.csv'\n",
    "\n",
    "def append_data_to_csv(filename, data):\n",
    "    df_new = pd.DataFrame(data)\n",
    "    if os.path.exists(filename):\n",
    "        df_existing = pd.read_csv(filename, quotechar='\"')\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "        df_combined.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "    df_combined.to_csv(filename, index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "def get_data():\n",
    "    for i in range(5):  #Attempt to get the data 5 times before printing the error message\n",
    "        try:\n",
    "            response_posts = requests.get(posts_url, headers=headers_posts, params=posts_params)\n",
    "            #We check the response code, if its not a \"200\", we break the loop and print the error code.\n",
    "            if response_posts.status_code == 200:\n",
    "                response_posts_json = response_posts.json()\n",
    "                print(\"Request successful!\")\n",
    "            else:\n",
    "                print(f\"Request unsuccessful. Status code: {response_posts.status_code}\")\n",
    "                break \n",
    "\n",
    "            #Adding another layer of protection in case the request is succesfull but the response format has changed or something like that.\n",
    "            if 'elements' in response_posts_json:\n",
    "                posts = response_posts_json['elements']\n",
    "            else:\n",
    "                print(\"No 'elements' key in the response. Check the API or the status of the request.\")\n",
    "                return\n",
    "                \n",
    "            fields_to_extract = ['createdAt', 'author', 'id', 'visibility']\n",
    "            content_fields_to_extract = ['description', 'thumbnail', 'source', 'title']\n",
    "            \n",
    "            data = []\n",
    "            for post in posts:\n",
    "                post_data = {field: post.get(field) for field in fields_to_extract}\n",
    "                content = post.get('content', {})\n",
    "                if content:\n",
    "                    article = content.get('article', {})\n",
    "                    post_data.update({f'content_{field}': article.get(field, None) for field in content_fields_to_extract})\n",
    "                data.append(post_data)\n",
    "            \n",
    "            append_data_to_csv(filename, data)\n",
    "            \n",
    "            break  # If the request was successful, break the loop\n",
    "        except requests.exceptions.RequestException:\n",
    "            time.sleep(2 ** i)  # Wait 2^i seconds before trying again\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response_posts.status_code}: {response_posts.text}\")\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    file_time = os.path.getmtime(filename)\n",
    "    file_datetime = datetime.datetime.fromtimestamp(file_time)\n",
    "\n",
    "    if (datetime.datetime.now() - file_datetime).total_seconds() > 60:  # Check if time difference is greater than 1 minute\n",
    "        get_data()\n",
    "    else:\n",
    "        posts_df = pd.read_csv(filename, quotechar='\"')\n",
    "else:\n",
    "    get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7826663",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#This version runs the function get_data if the time difference between now and the file date is needed in minutes\n",
    "if os.path.exists(filename):\n",
    "    file_time = os.path.getmtime(filename)\n",
    "    file_datetime = datetime.datetime.fromtimestamp(file_time)\n",
    "\n",
    "    if (datetime.datetime.now() - file_datetime).total_seconds() > 60:  # Check if time difference is greater than 1 minute\n",
    "        get_data()\n",
    "    else:\n",
    "        posts_df = pd.read_csv(filename)\n",
    "\n",
    "else:\n",
    "    get_data()\n",
    "    \n",
    "#This version runs the function get_data if the time difference between now and the file date is needed in days \n",
    "\n",
    "if os.path.exists(filename):\n",
    "    file_time = os.path.getmtime(filename)\n",
    "    file_datetime = datetime.datetime.fromtimestamp(file_time)\n",
    "\n",
    "    if (datetime.datetime.now() - file_datetime).days > 3: # Here we set the amount of days ie. if we only want to update every x days.\n",
    "        get_data()\n",
    "    else:\n",
    "        posts_df = pd.read_csv(filename)\n",
    "\n",
    "else:\n",
    "    get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7c30d",
   "metadata": {},
   "source": [
    "## We now have a list of Post Ids that we will use to retrieve metadata per post ID.\n",
    "\n",
    "Its important to understand that the API responds with all those posts that appear on the wall, that includes shares and internal content. We have designed a function to separate and save internal, external and full content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ac925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.714357Z",
     "start_time": "2023-06-30T11:19:51.688472Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "filename=\"list_posts.csv\"\n",
    "\n",
    "def append_new_data(filename, df_new):\n",
    "    if os.path.exists(filename):\n",
    "        df_existing = pd.read_csv(filename)\n",
    "        df_combined = pd.concat([df_existing, df_new])\n",
    "        df_combined.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "    df_combined.to_csv(filename, index=False)\n",
    "\n",
    "def process_data():\n",
    "    posts_internal_external = pd.read_csv(filename)\n",
    "\n",
    "    share_pattern = r'urn:li:share:.+'\n",
    "    ugcPost_pattern = r'urn:li:ugcPost:.+'\n",
    "\n",
    "    mask_share = posts_internal_external['id'].str.match(share_pattern)\n",
    "    mask_ugcPost = posts_internal_external['id'].str.match(ugcPost_pattern)\n",
    "\n",
    "    df_share_new = posts_internal_external[mask_share]\n",
    "    df_ugcPost_new = posts_internal_external[mask_ugcPost]\n",
    "\n",
    "    append_new_data('shares.csv', df_share_new)\n",
    "    append_new_data('UGCpost.csv', df_ugcPost_new)\n",
    "    \n",
    "process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd2e4",
   "metadata": {},
   "source": [
    "## Lets load the required csv that holds the Id's we need metadata from.\n",
    "\n",
    "Note several things:\n",
    "- `If we want only organic post data`:\n",
    "    - load the df with csv file name \"UGCpost.csv\"\n",
    "- `If we want only shared post data`:\n",
    "    - - load the df with csv file name \"shares.csv\"\n",
    "- `If we want everything together`:\n",
    "    - - load the df with csv file name \"list_posts.csv\"\n",
    "\n",
    "There can only be 1 of the 3 cells unlocked at once. 2 must be frozen for the script to work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18642e8c",
   "metadata": {},
   "source": [
    "### Only UGC posts (internal posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2076cf1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('UGCpost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8da588",
   "metadata": {},
   "source": [
    "### Only share posts (shared posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b36682",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('shares.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac97c3",
   "metadata": {},
   "source": [
    "### All post (shares and ugc posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbcbcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.719184Z",
     "start_time": "2023-06-30T11:19:51.714865Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('list_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab6ee0",
   "metadata": {},
   "source": [
    "## We now convert the timestamps we receive from the API request to a format that is readable by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c7b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.723407Z",
     "start_time": "2023-06-30T11:19:51.720193Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "posts_df['createdAt'] = pd.to_datetime(posts_df['createdAt'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0247e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.727062Z",
     "start_time": "2023-06-30T11:19:51.723407Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "list_post_id = posts_df['id'].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff4aa9",
   "metadata": {},
   "source": [
    "## Setting up the headers, parameters, and API URL to get all the post metadata\n",
    "\n",
    "Note several things:\n",
    "- `metadata_headers`:\n",
    "    - Don't change\n",
    "\n",
    "- `metadata_url`:\n",
    "    - This is the endpoint to retrieve data on posts (not reactions, just posts themselves), we will put this in a for loop to request all the list at the same time.\n",
    "- `LinkedIn-Version`:\n",
    "    - This is the API version. This might change with new endpoints needing a newer version, in any case we must include a valid version in the header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cdb594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.730136Z",
     "start_time": "2023-06-30T11:19:51.728081Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "metadata_headers = {\n",
    "    'Authorization': f'Bearer {access_token}',\n",
    "    'Connection': 'Keep-Alive',\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"LinkedIn-Version\": \"202301\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8d37c",
   "metadata": {},
   "source": [
    "## Now we have the code to iterate through the post_id list and record the social reactions in a dataframe\n",
    "\n",
    "We have also added functionality that downloads this dataframe to a csv file and for future runs, loads this file and only runs requests on posts we dont have. This is a temporary measure because were not allowed unlimited requests by the API. If we move to production, we can delete the reading of saved csv and request new data every day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a698878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.748577Z",
     "start_time": "2023-06-30T11:19:51.734147Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'post_with_metadata_response.csv'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    df_responses = pd.read_csv(filename)\n",
    "else:\n",
    "    df_responses = pd.DataFrame(columns=['id', 'response'])\n",
    "    \n",
    "fetched_ids = df_responses['id'].tolist()\n",
    "new_post_ids = [id for id in list_post_id if id not in fetched_ids]\n",
    "    \n",
    "if not new_post_ids:  # Check if the list is empty\n",
    "    print(\"All the posts have metadata, there's no need to request any additional information.\")\n",
    "else:\n",
    "    counter = 0\n",
    "    for post_id in new_post_ids:\n",
    "        metadata_url = f'https://api.linkedin.com/rest/socialMetadata/{post_id}'\n",
    "        metadata_response = requests.get(metadata_url, headers=metadata_headers)\n",
    "\n",
    "        if metadata_response.status_code == 200:\n",
    "            response_data = metadata_response.json()\n",
    "            df_responses.loc[len(df_responses)] = {'id': post_id, 'response': response_data}\n",
    "            counter += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve metadata for post ID: {post_id}, you've requested {counter} posts today and you probably reached maximum API quota for today.\")\n",
    "            break \n",
    "\n",
    "    df_responses.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe27b19",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## We are going to adjust the newly created dataframe to have the data correctly structured\n",
    "\n",
    "Our code from before will just update the csv with 2 columns, id and response. In this code, we take that csv and structure it correctly then overwrite the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b522b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.754359Z",
     "start_time": "2023-06-30T11:19:51.749584Z"
    }
   },
   "outputs": [],
   "source": [
    "df_responses = pd.read_csv('post_with_metadata_response.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccf5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.760747Z",
     "start_time": "2023-06-30T11:19:51.754869Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "if isinstance(df_responses.loc[0, 'response'], str):\n",
    "    import json\n",
    "    df_responses['response'] = df_responses['response'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\")))\n",
    "\n",
    "df_responses_normalized = pd.json_normalize(df_responses['response'])\n",
    "df_responses_with_post_id = pd.concat([df_responses['id'], df_responses_normalized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b77b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.766603Z",
     "start_time": "2023-06-30T11:19:51.761752Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['entity', 'reactionSummaries.PRAISE.reactionType',\n",
    "                   'reactionSummaries.APPRECIATION.reactionType',\n",
    "                   'reactionSummaries.LIKE.reactionType',\n",
    "                   'reactionSummaries.INTEREST.reactionType',\n",
    "                   'reactionSummaries.EMPATHY.reactionType',\n",
    "                   'reactionSummaries.ENTERTAINMENT.reactionType',\n",
    "                   'reactionSummaries.MAYBE.reactionType']\n",
    "\n",
    "for column in columns_to_drop:\n",
    "    if column in df_responses_with_post_id.columns:\n",
    "        df_responses_with_post_id.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06e5e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.771710Z",
     "start_time": "2023-06-30T11:19:51.767612Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "column_rename_mapping = {\n",
    "    'reactionSummaries.PRAISE.count': 'Praise Reaction Count',\n",
    "    'reactionSummaries.APPRECIATION.count': 'Appreciation Reaction Count',\n",
    "    'reactionSummaries.LIKE.count': 'Like Reaction Count',\n",
    "    'reactionSummaries.INTEREST.count': 'Interest Reaction Count',\n",
    "    'reactionSummaries.EMPATHY.count': 'Empathy Reaction Count',\n",
    "    'reactionSummaries.ENTERTAINMENT.count': 'Entertainment Reaction Count',\n",
    "    'commentSummary.count': 'Comment Count',\n",
    "    'commentSummary.topLevelCount': 'Top Level Comment Count',\n",
    "    'reactionSummaries.MAYBE.count': 'Maybe Reaction Count'\n",
    "}\n",
    "\n",
    "for old_column, new_column in column_rename_mapping.items():\n",
    "    if old_column in df_responses_with_post_id.columns:\n",
    "        df_responses_with_post_id.rename(columns={old_column: new_column}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3783794",
   "metadata": {},
   "source": [
    "## We will format the data further and then merge dataframes to have all the data in one csv file\n",
    "\n",
    "- **First**\n",
    "  - We merge post metadata with post data\n",
    "\n",
    "- **Second**\n",
    "  - Check if theres difference between the loaded metadata csv and the one created by merging the two previous dataframes\n",
    "\n",
    "- **Third**\n",
    "  - If a change is detected, we overwrite with the newest information, if the dfs are the same, we do nothing.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e2b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T11:19:51.787450Z",
     "start_time": "2023-06-30T11:19:51.772717Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Assuming your merged dataframe\n",
    "metadata_per_post = pd.merge(posts_df, df_responses_with_post_id, on='id', how='outer')\n",
    "\n",
    "# Specify file path\n",
    "file_path = \"Raw Data CSVs//metadata_per_post.csv\" \n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Load the existing data\n",
    "    existing_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Check for differences\n",
    "    if not metadata_per_post.equals(existing_df):\n",
    "        # If differences are found, update (overwrite) the file\n",
    "        metadata_per_post.to_csv(file_path, index=False)\n",
    "        print(\"CSV file updated.\")\n",
    "    else:\n",
    "        print(\"No differences found. CSV file not updated.\")\n",
    "else:\n",
    "    # If the file doesn't exist, create it\n",
    "    metadata_per_post.to_csv(file_path, index=False)\n",
    "    print(\"CSV file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3064fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.85,
   "position": {
    "height": "144.45px",
    "left": "1003.6px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
